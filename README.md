# Document Analyzer App (Using Streamlit)

**Models Used**:sentence transformers and Llama 3.2-3B served via Ollama.  

This project is a Streamlit web application that allows you to chat with your documents. You can upload a PDF, DOCX, or TXT file, and the application will index its content, allowing you to ask questions and receive answers based on the document's information.

The question-answering is powered by a locally running **Llama 3.2-3B** model, served via the Ollama server API. The embedding model used is **`sentence-transformers/all-MiniLM-L6-v2`**.

## How It Works

The application follows a RAG (Retrieval-Augmented Generation) architecture:

1.  **Document Upload & Processing**: The user uploads a document through the Streamlit interface. The text is extracted and split into smaller, overlapping chunks.
2.  **Embedding & Indexing**: Each text chunk is converted into a numerical vector (embedding) using the `sentence-transformers/all-MiniLM-L6-v2` model. These vectors are then stored in a high-speed FAISS vector index on the local disk.
3.  **User Query**: The user asks a question in the chat interface.
4.  **Similarity Search**: The user's question is also converted into an embedding. The FAISS index is then searched to find the text chunks from the document that are most semantically similar to the question.
5.  **LLM Answering**: These relevant text chunks (the "context") and the original question are sent in a formatted prompt to the Ollama API endpoint.
6.  **Display Answer**: The LLaMA 3.2 model generates an answer based on the provided context, which is then displayed to the user.

---

## Prerequisites

Before you begin, you must have the **Ollama Server** installed and running with the correct model.

#### Setting up Ollama and Llama 3.2-3B (Step-by-Step)

1.  **Download and Install Ollama**:
    * Go to the official Ollama website: <https://ollama.com>
    * Download and run the installer for your operating system (Windows, macOS, or Linux).

2.  **Pull the Llama 3.2 Model**:
    * Once Ollama is installed, open your terminal (Command Prompt, PowerShell, or Terminal on Mac/Linux).
    * Run the following command to download and set up the `llama3.2:3b` model:
        ```bash
        ollama run llama3.2:3b
        ```
    * This command will download the model and start the Ollama server. You can type `/bye` to exit the prompt once it's ready.

3.  **Ensure the Ollama Server is Running**:
    * The Ollama application must be running in the background. On Windows and macOS, it usually runs automatically and has an icon in your system tray or menu bar.
    * You can verify the model is available by running `ollama list` in your terminal. You should see `llama3.2:3b` in the output.

---

## Installation & Setup

Follow these steps to set up the Python environment and run the application.

1.  **Clone or Download the Code**:
    * Download all the project files (`interface.py`, `indexing.py`, `llm_query.py`, `requirements.txt`) into a single folder on your computer.

2.  **Create a Python Virtual Environment**:
    * Open a terminal in your project folder and run the following commands:
        ```bash
        # Create a virtual environment
        python -m venv venv
        
        # Activate the virtual environment
        # On Windows:
        .\venv\Scripts\activate
        # On macOS/Linux:
        source venv/bin/activate
        ```

3.  **Install Dependencies**:
    * With your virtual environment active, install all the required Python libraries:
        ```bash
        pip install -r requirements.txt
        ```

---

## Running the Application

1.  Make sure your Ollama server is running.
2.  Ensure your virtual environment is active (`(venv)` should be in your terminal prompt).
3.  Run the following command in your terminal:
    ```bash
    streamlit run interface.py
    ```
4.  The application will open in a new tab in your web browser.

---

## How to Use the App

1.  Use the "Upload a document" button to select a PDF, DOCX, or TXT file.
    ![Alt text for the image](images/ss1.png)
2.  Once uploaded, click the "Process and Index Document" button and wait for the success message.
    ![Alt text for the image](images/ss2new.png)
3.  Type your question into the text input box at the bottom and press Enter.
    ![Alt text for the image](images/ss3.png)
4.  The answer generated by LLaMA will appear in the chat.
    ![Alt text for the image](images/ss4.png)
5.  You can view the exact context chunks used for the answer by expanding the "Context Used" section.
    ![Alt text for the image](images/ss1.png)
---

## File Structure

* `interface.py`: The main file that runs the Streamlit web interface and handles user interaction.
* `indexing.py`: Contains functions for document processing (text extraction, chunking, embedding, FAISS indexing).
* `llm_query.py`: Contains functions for querying the index and making API requests to the Ollama server.
* `requirements.txt`: A list of all required Python packages.

#Document Analyzer App (using Tkinter)

This project presents an **offline-capable Document Analyzer App** built using Python's `tkinter` for the graphical user interface, enabling users to chat with their documents locally. You can upload PDF, DOCX, or TXT files, and the application will process, index, and allow you to query their content.

The application uses a **local Llama 3.2-3B model** for question-answering, served via the Ollama server API. For embedding text, it utilizes the `sentence-transformers/all-MiniLM-L6-v2` model, loaded directly through the `transformers` library.

## How It Works

The application employs a **RAG (Retrieval-Augmented Generation) architecture**:

1.  **Document Upload & Processing**: Users upload a document through the `tkinter` interface. The text is extracted from the document and then split into smaller, overlapping chunks.
2.  **Embedding & Indexing**: Each text chunk is converted into a numerical vector (embedding) using the `sentence-transformers/all-MiniLM-L6-v2` model, loaded and run locally. These vectors are then stored in a high-speed **FAISS vector index** on the local disk.
3.  **User Query**: The user asks a question in the chat interface.
4.  **Similarity Search**: The user's question is also converted into an embedding using the same local embedding model. The FAISS index is then searched to find the text chunks from the document that are most semantically similar to the question.
5.  **LLM Answering**: These relevant text chunks (the "context") and the original question are sent in a formatted prompt to the **locally running Ollama API endpoint**.
6.  **Display Answer**: The LLaMA 3.2 model generates an answer based on the provided context, which is then displayed to the user in the chat interface.

## Prerequisites

Before running the application, you must have the Ollama Server installed and running with the correct model.

### Setting up Ollama and Llama 3.2-3B (Step-by-Step)

1.  **Download and Install Ollama**:
    * Go to the official Ollama website: [https://ollama.com](https://ollama.com)
    * Download and run the installer for your operating system (Windows, macOS, or Linux).

2.  **Pull the Llama 3.2 Model**:
    * Once Ollama is installed, open your terminal (Command Prompt, PowerShell, or Terminal on Mac/Linux).
    * Run the following command to download and set up the `llama3.2:3b` model:
        ```bash
        ollama run llama3.2:3b
        ```
    * This command will download the model and start the Ollama server. You can type `/bye` to exit the prompt once it's ready.

3.  **Ensure the Ollama Server is Running**:
    * The Ollama application must be running in the background. On Windows and macOS, it usually runs automatically and has an icon in your system tray or menu bar.
    * You can verify the model is available by running `ollama list` in your terminal. You should see `llama3.2:3b` in the output.
  ## Installation & Setup

Follow these steps to set up the Python environment and run the application.

1.  **Download the Code**:
    * Download all the project files (`interface.py`, `indexing.py`, `llm_query.py`, `core_utils.py`, `requirements.txt`) into a single folder on your computer.

2.  **Create a Python Virtual Environment**:
    * Open a terminal in your project folder and run the following commands:
        ```bash
        # Create a virtual environment
        python -m venv venv

        # Activate the virtual environment
        # On Windows:
        .\venv\Scripts\activate
        # On macOS/Linux:
        source venv/bin/activate
        ```

3.  **Install Dependencies**:
    * With your virtual environment active, install all the required Python libraries:
        ```bash
        pip install -r requirements.txt
        ```

  ## Running the Application

  1.  Make sure your **Ollama server is running**.
  2.  Ensure your **virtual environment is active** (`(venv)` should be in your terminal prompt).
  3.  Run the following command in your terminal:
    ```bash
    python interface.py
    ```
    The application will open as a `tkinter` window.

 ## How to Use the App

  1.  Use the "**Browse Files**" button to select a PDF, DOCX, or TXT file.
       ![Alt text for the image] (images/s.1.png)
  2.  Once uploaded, click the "**Process and Index Document**" button and wait for the success message to appear in the status label.
       ![Alt text for the image] (images/s.2.png)
  3.  Type your question into the text input box at the bottom and press **Enter** or click the "**Ask**" button.
       ![Alt text for the image] (images/s.3.png)
  4.  The answer generated by LLaMA will appear in the chat display.
     
  5.  You can view the exact context chunks used for the answer by observing the "[Context Used]" section below the bot's response in the chat display.

 ## File Structure

 * **`interface.py`**: The main file that runs the `tkinter` graphical user interface and handles user interaction.
 * **`indexing.py`**: Contains functions specifically for the document indexing process, including saving the FAISS index. It calls `core_utils` for text       processing and embedding.
 * **`llm_query.py`**: Contains functions for querying the FAISS index and making API requests to the Ollama server to get answers from the LLM. It also uses  `core_utils` for embedding the query.
 * **`core_utils.py`**: A new utility file containing reusable functions for text extraction, text chunking, and text embedding that are shared by both the  `indexing` and `llm_query` modules.
 * **`requirements.txt`**: A list of all required Python packages.
